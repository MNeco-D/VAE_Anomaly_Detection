{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Variational Autoencoder (VAE) for Anomaly Detection\n",
                "\n",
                "This notebook implements a VAE for anomaly detection with the following specific features:\n",
                "1.  **Synthetic Data**: High-dimensional data with a **distinct anomaly cluster**.\n",
                "2.  **VAE Architecture**: PyTorch implementation with reparameterization.\n",
                "3.  **Hyperparameter Sweep**: Analyzing impact of **$\\beta$** (KL weight) and **Latent Dimension**.\n",
                "4.  **Advanced Evaluation**: AUC-ROC, **AUC-PR**, and **Optimal Threshold Selection** based on F1-score."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import DataLoader, TensorDataset\n",
                "from sklearn.datasets import make_blobs\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import MinMaxScaler\n",
                "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score, f1_score, classification_report\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Set random seed\n",
                "SEED = 42\n",
                "np.random.seed(SEED)\n",
                "torch.manual_seed(SEED)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Generation: Distinct Anomaly Cluster\n",
                "\n",
                "We generate 5000 samples with 10 features.\n",
                "- **Normal Data**: Generated from a mixture of 3 Gaussian blobs.\n",
                "- **Anomalies (2-3%)**: Generated from a **separate, distinct Gaussian cluster** (shifted mean) to ensure they form a coherent group rather than just random noise."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_synthetic_data(n_samples=5000, n_features=20, anomaly_ratio=0.03, random_state=42):\n",
                "    np.random.seed(random_state)\n",
                "    \n",
                "    n_anomalies = int(n_samples * anomaly_ratio)\n",
                "    n_normal = n_samples - n_anomalies\n",
                "    \n",
                "    # 1. Normal Data: 3 Centers\n",
                "    X_normal, _ = make_blobs(n_samples=n_normal, n_features=n_features, centers=3, cluster_std=1.0, random_state=random_state)\n",
                "    \n",
                "    # 2. Anomalies: A Distinct Cluster\n",
                "    # We shift the mean significantly to create a separate cluster\n",
                "    anomaly_mean = 4.0  # Shifted away from standard blob centers (usually -10 to 10)\n",
                "    X_anomalies = np.random.normal(loc=anomaly_mean, scale=1.5, size=(n_anomalies, n_features))\n",
                "    \n",
                "    # Combine\n",
                "    X = np.vstack([X_normal, X_anomalies])\n",
                "    y = np.hstack([np.zeros(n_normal), np.ones(n_anomalies)])\n",
                "    \n",
                "    # Scaling\n",
                "    scaler = MinMaxScaler()\n",
                "    X_scaled = scaler.fit_transform(X)\n",
                "    \n",
                "    # Split\n",
                "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, shuffle=True, stratify=y, random_state=random_state)\n",
                "    \n",
                "    # To Tensors\n",
                "    X_train_tensor = torch.FloatTensor(X_train)\n",
                "    X_test_tensor = torch.FloatTensor(X_test)\n",
                "    y_test_tensor = torch.FloatTensor(y_test)\n",
                "    \n",
                "    return X_train_tensor, X_test_tensor, y_test_tensor, n_features\n",
                "\n",
                "# Generate Data\n",
                "X_train, X_test, y_test, N_FEATURES = generate_synthetic_data()\n",
                "\n",
                "# DataLoaders\n",
                "BATCH_SIZE = 64\n",
                "train_loader = DataLoader(TensorDataset(X_train, X_train), batch_size=BATCH_SIZE, shuffle=True)\n",
                "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=BATCH_SIZE, shuffle=False)\n",
                "\n",
                "print(f\"Data Ready. Features: {N_FEATURES}. Anomalies: {y_test.sum()} / {len(y_test)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. VAE Architecture\n",
                "\n",
                "Standard VAE with `reparameterize` method. Loss is customizable via `beta`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class VAE(nn.Module):\n",
                "    def __init__(self, input_dim, hidden_dim=64, latent_dim=10):\n",
                "        super(VAE, self).__init__()\n",
                "        \n",
                "        # Encoder\n",
                "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
                "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
                "        \n",
                "        self.fc_mu = nn.Linear(hidden_dim // 2, latent_dim)\n",
                "        self.fc_logvar = nn.Linear(hidden_dim // 2, latent_dim)\n",
                "        \n",
                "        # Decoder\n",
                "        self.fc3 = nn.Linear(latent_dim, hidden_dim // 2)\n",
                "        self.fc4 = nn.Linear(hidden_dim // 2, hidden_dim)\n",
                "        self.fc5 = nn.Linear(hidden_dim, input_dim)\n",
                "        \n",
                "    def encode(self, x):\n",
                "        h1 = F.relu(self.fc1(x))\n",
                "        h2 = F.relu(self.fc2(h1))\n",
                "        return self.fc_mu(h2), self.fc_logvar(h2)\n",
                "    \n",
                "    def reparameterize(self, mu, logvar):\n",
                "        std = torch.exp(0.5 * logvar)\n",
                "        eps = torch.randn_like(std)\n",
                "        return mu + eps * std\n",
                "    \n",
                "    def decode(self, z):\n",
                "        h3 = F.relu(self.fc3(z))\n",
                "        h4 = F.relu(self.fc4(h3))\n",
                "        return torch.sigmoid(self.fc5(h4))\n",
                "    \n",
                "    def forward(self, x):\n",
                "        mu, logvar = self.encode(x)\n",
                "        z = self.reparameterize(mu, logvar)\n",
                "        recon_x = self.decode(z)\n",
                "        return recon_x, mu, logvar\n",
                "\n",
                "def loss_function(recon_x, x, mu, logvar, beta=1.0):\n",
                "    BCE = F.mse_loss(recon_x, x, reduction='sum')\n",
                "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
                "    return BCE + beta * KLD, BCE, KLD"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Hyperparameter Sweep & Training\n",
                "\n",
                "We sweep over:\n",
                "- **Beta**: `[0.1, 1.0, 5.0]`\n",
                "- **Latent Dim**: `[2, 5, 10]`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "\n",
                "def train_model(beta, latent_dim, epochs=20):\n",
                "    model = VAE(input_dim=N_FEATURES, hidden_dim=32, latent_dim=latent_dim).to(DEVICE)\n",
                "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
                "    model.train()\n",
                "    \n",
                "    for epoch in range(epochs):\n",
                "        for data, _ in train_loader:\n",
                "            data = data.to(DEVICE)\n",
                "            optimizer.zero_grad()\n",
                "            recon, mu, logvar = model(data)\n",
                "            loss, _, _ = loss_function(recon, data, mu, logvar, beta=beta)\n",
                "            loss.backward()\n",
                "            optimizer.step()\n",
                "    return model\n",
                "\n",
                "def evaluate_model(model, test_loader):\n",
                "    model.eval()\n",
                "    mse_scores = []\n",
                "    labels = []\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for data, target in test_loader:\n",
                "            data = data.to(DEVICE)\n",
                "            recon, _, _ = model(data)\n",
                "            loss_per_sample = torch.sum((recon - data)**2, dim=1)\n",
                "            mse_scores.extend(loss_per_sample.cpu().numpy())\n",
                "            labels.extend(target.cpu().numpy())\n",
                "            \n",
                "    return np.array(mse_scores), np.array(labels)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Results & Analysis\n",
                "\n",
                "We iterate through all configurations, calculate metrics, and find the optimal threshold for the best config."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "results = []\n",
                "BETAS = [0.1, 1.0, 5.0]\n",
                "LATENT_DIMS = [2, 5, 10]\n",
                "\n",
                "print(f\"{'Beta':<6} | {'Latent':<6} | {'AUC-ROC':<8} | {'AUC-PR':<8} | {'Best F1':<8}\")\n",
                "print(\"-\"*50)\n",
                "\n",
                "best_f1_global = 0\n",
                "best_config = None\n",
                "best_scores = None\n",
                "best_labels = None\n",
                "\n",
                "for beta in BETAS:\n",
                "    for ld in LATENT_DIMS:\n",
                "        model = train_model(beta, ld)\n",
                "        scores, labels = evaluate_model(model, test_loader)\n",
                "        \n",
                "        # Metrics\n",
                "        roc_auc = mult_auc_roc = 0\n",
                "        fpr, tpr, _ = roc_curve(labels, scores)\n",
                "        roc_auc = auc(fpr, tpr)\n",
                "        \n",
                "        precision, recall, thresholds = precision_recall_curve(labels, scores)\n",
                "        auc_pr = average_precision_score(labels, scores)\n",
                "        \n",
                "        # Find Best F1 for this Model\n",
                "        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
                "        best_f1_local = np.max(f1_scores)\n",
                "        \n",
                "        print(f\"{beta:<6} | {ld:<6} | {roc_auc:.4f}   | {auc_pr:.4f}   | {best_f1_local:.4f}\")\n",
                "        \n",
                "        results.append({\n",
                "            'beta': beta,\n",
                "            'latent_dim': ld,\n",
                "            'auc_roc': roc_auc,\n",
                "            'auc_pr': auc_pr,\n",
                "            'best_f1': best_f1_local\n",
                "        })\n",
                "        \n",
                "        if best_f1_local > best_f1_global:\n",
                "            best_f1_global = best_f1_local\n",
                "            best_config = (beta, ld)\n",
                "            best_scores = scores\n",
                "            best_labels = labels\n",
                "\n",
                "print(f\"\\nBest Configuration: Beta={best_config[0]}, Latent Dim={best_config[1]} with F1={best_f1_global:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Optimal Threshold & Final Visualization\n",
                "\n",
                "For the best performing model, we visualize the separation and the trade-offs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Re-calculate precision-recall for best model to get threshold\n",
                "precision, recall, thresholds = precision_recall_curve(best_labels, best_scores)\n",
                "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
                "best_idx = np.argmax(f1_scores)\n",
                "optimal_threshold = thresholds[best_idx]\n",
                "\n",
                "print(f\"Optimal Anomaly Threshold: {optimal_threshold:.6f}\")\n",
                "\n",
                "# Print Classification Report at Optimal Threshold\n",
                "predicted_labels = (best_scores > optimal_threshold).astype(int)\n",
                "print(\"\\nClassification Report:\")\n",
                "print(classification_report(best_labels, predicted_labels, target_names=['Normal', 'Anomaly']))\n",
                "\n",
                "plt.figure(figsize=(12, 5))\n",
                "\n",
                "# 1. Reconstruction Error Histogram\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.hist(best_scores[best_labels==0], bins=50, alpha=0.6, label=\"Normal\", color='blue', density=True)\n",
                "plt.hist(best_scores[best_labels==1], bins=50, alpha=0.6, label=\"Anomaly\", color='red', density=True)\n",
                "plt.axvline(optimal_threshold, color='k', linestyle='--', label=f'Threshold={optimal_threshold:.3f}')\n",
                "plt.title(\"Reconstruction Error Distribution\")\n",
                "plt.xlabel(\"MSE Loss\")\n",
                "plt.legend()\n",
                "\n",
                "# 2. Precision-Recall Curve\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.plot(recall, precision, marker='.', label='VAE')\n",
                "plt.scatter(recall[best_idx], precision[best_idx], marker='o', color='black', label='Best F1')\n",
                "plt.title(f\"PR Curve (AUC={average_precision_score(best_labels, best_scores):.3f})\")\n",
                "plt.xlabel(\"Recall\")\n",
                "plt.ylabel(\"Precision\")\n",
                "plt.legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
