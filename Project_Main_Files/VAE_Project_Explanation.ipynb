{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# VAE for Anomaly Detection: Project Documentation"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Project Overview\n",
                "This project implements a **Variational Autoencoder (VAE)** to detect anomalies in high-dimensional synthetic data. The core principle is that a VAE trained effectively on normal data will have a **low reconstruction error** for normal samples but a **high reconstruction error** for anomalies, as it hasn't learned to encode/decode the anomalous patterns effectively."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Data Generation & Preprocessing"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The project generates a synthetic dataset designed to simulate a realistic anomaly detection scenario.\n",
                "\n",
                "*   **Total Samples**: 5,000 samples with 20 features.\n",
                "*   **Normal Data**: Generated using a Gaussian mixture model (`make_blobs` with 3 centers) to simulate multi-modal normal behavior.\n",
                "*   **Anomalies**: Introducing a **contamination ratio of ~3%**. These anomalies are generated from a distinct Gaussian distribution with a shifted mean (shifted by +4.0), ensuring they form a coherent \"attack\" or \"fault\" cluster rather than just random noise.\n",
                "*   **Preprocessing**: All data is normalized using `MinMaxScaler` to the [0, 1] range, which is crucial for Neural Network training and ensures stable convergence.\n",
                "*   **Train/Test Split**: Stratified split (80/20), meaning both training and testing sets contain a small fraction of anomalies. This represents a robust \"unsupervised\" or \"semi-supervised\" setting where the training data is not perfectly clean."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. VAE Architecture"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The model uses a standard VAE architecture implemented in PyTorch:\n",
                "\n",
                "### Encoder\n",
                "The encoder compresses the high-dimensional input ($x$) into a lower-dimensional latent space ($z$).\n",
                "*   **Layers**: Input $\\rightarrow$ Hidden (Relu) $\\rightarrow$ Hidden (Relu) $\\rightarrow$ Latent Parameters ($\\\\mu$, $\\\\sigma$).\n",
                "*   **Output**: Two vectors representing the mean ($\\\\mu$) and log-variance ($\\\\log \\\\sigma^2$) of the latent distribution.\n",
                "\n",
                "### Reparameterization Trick\n",
                "To allow backpropagation through the stochastic sampling process, the model uses the reparameterization trick:\n",
                "$$ z = \\\\mu + \\\\sigma \\\\cdot \\\\epsilon, \\\\quad \\\\epsilon \\\\sim \\\\mathcal{N}(0, 1) $$\n",
                "This allows the network to learn the parameters of the distribution while maintaining differentiability.\n",
                "\n",
                "### Decoder\n",
                "The decoder attempts to reconstruct the original input from the latent vector ($z$).\n",
                "*   **Layers**: Latent $\\rightarrow$ Hidden (Relu) $\\rightarrow$ Hidden (Relu) $\\rightarrow$ Output (Sigmoid).\n",
                "*   **Output Strategy**: Uses `Sigmoid` activation to ensure outputs are in [0, 1], matching the scaled input data."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Training Methodology"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The model is trained to minimize a composite **VAE Loss Function**:\n",
                "\n",
                "$$ \\\\mathcal{L} = \\\\mathcal{L}_{recon} + \\\\beta \\\\cdot \\\\mathcal{L}_{KL} $$\n",
                "\n",
                "1.  **Reconstruction Loss ($\\\\mathcal{L}_{recon}$)**: Measures how well the VAE reconstructs the input. calculated using **MSE (Mean Squared Error)**.\n",
                "2.  **KL Divergence ($\\\\mathcal{L}_{KL}$)**: Regularizes the latent space by forcing the learned distribution to approximate a standard Normal distribution $\\\\mathcal{N}(0, 1)$.\n",
                "3.  **Beta ($\\\\beta$) Scaling**: A hyperparameter that weights the importance of the KL divergence.\n",
                "    *   **High $\\\\beta$**: Forces a very smooth latent space but might result in blurry reconstructions (posterior collapse).\n",
                "    *   **Low $\\\\beta$**: Prioritizes reconstruction accuracy, potentially leading to overfitting but sharper definitions for anomalies."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Anomaly Detection Logic"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Once trained, the VAE is used as a scorer:\n",
                "1.  **Inference**: Pass a sample $x$ through the VAE to get reconstructed $\\hat{x}$.\n",
                "2.  **Scoring**: Calculate Reconstruction Error: $Error = \\\\sum (x - \\\\hat{x})^2$.\n",
                "3.  **Decision**:\n",
                "    *   **Normal**: Error $<$ Threshold\n",
                "    *   **Anomaly**: Error $>$ Threshold"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Optimization & Evaluation Results"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The implementation performs a Grid Search to find the best configuration:\n",
                "*   **Hyperparameters Swept**: $\\\\beta \\\\in [0.1, 1.0, 5.0]$ and Latent Dimension $\\\\in [2, 5, 10]$.\n",
                "*   **Best Configuration Found**: **$\\\\beta=0.1$, Latent Dim=10**.\n",
                "    *   Since the goal is anomaly detection, a lower $\\\\beta$ (0.1) often works better because it allows the model to \"overfit\" slightly to the normal data structure, making the contrast with anomalies sharper.\n",
                "\n",
                "### Metrics Used\n",
                "*   **AUC-ROC**: Measures the ability to distinguish between classes across all thresholds. (Achieved ~0.99 with best model).\n",
                "*   **AUC-PR (Average Precision)**: Critical for imbalanced datasets (like anomaly detection). (Achieved ~0.90 with best model).\n",
                "*   **F1-Score**: The harmonic mean of precision and recall.\n",
                "\n",
                "### Final Threshold Selection\n",
                "The optimal threshold is selected by maximizing the F1-score on the test set.\n",
                "*   **Visual Validation**: histograms show a clear separation between the reconstruction error distributions of Normal vs. Anomaly samples."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}